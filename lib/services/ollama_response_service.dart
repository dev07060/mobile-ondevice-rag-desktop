/// Service for generating LLM responses with RAG context.
/// Handles response mode selection (strict/hybrid/fallback) and prompt construction.

import 'package:flutter/foundation.dart';
import 'package:mobile_rag_engine/mobile_rag_engine.dart';
import 'package:ollama_dart/ollama_dart.dart';

/// Response mode based on RAG similarity scores
enum ResponseMode {
  strict, // High similarity (>= 0.7): use only document context
  hybrid, // Medium similarity (>= 0.5): combine document + general knowledge
  fallback, // Low/no similarity: use general knowledge
}

/// Result of response generation
class OllamaResponseResult {
  final String response;
  final ResponseMode mode;
  final double bestSimilarity;

  const OllamaResponseResult({
    required this.response,
    required this.mode,
    required this.bestSimilarity,
  });
}

/// Supported response languages for the LLM
enum ResponseLanguage { english, korean }

/// Service for generating Ollama LLM responses with RAG context
class OllamaResponseService {
  final OllamaClient ollamaClient;
  final String modelName;

  // Thresholds for response mode selection
  static const double hybridThreshold = 0.5;
  static const double strictThreshold = 0.7;

  OllamaResponseService({
    required this.ollamaClient,
    this.modelName = 'gemma3:4b',
  });

  /// Generate a response using RAG context
  /// Returns the response text and metadata
  /// [onToken] callback is called for each token as it arrives (for real-time UI)
  Future<OllamaResponseResult> generateResponse({
    required String query,
    required String contextText,
    required RagSearchResult ragResult,
    required bool hasRelevantContext,
    required List<Message> chatHistory,
    ResponseLanguage language = ResponseLanguage.english,
    void Function(Message)? onHistoryUpdate,
    void Function(String token)? onToken,
  }) async {
    // Calculate best similarity score for mode decision
    final bestSimilarity = _calculateBestSimilarity(ragResult);

    // Determine response mode
    final mode = _determineResponseMode(hasRelevantContext, bestSimilarity);

    debugPrint(
      'ğŸ¯ Response Mode: ${mode.name.toUpperCase()} '
      '(bestSim: ${bestSimilarity.toStringAsFixed(3)})',
    );

    try {
      // Build messages
      final messages = <Message>[];

      // 1. System Prompt - varies by mode and language
      messages.add(_buildSystemPrompt(mode, language));

      // 2. Chat History (last 6 messages)
      final historyStart = chatHistory.length > 6 ? chatHistory.length - 6 : 0;
      messages.addAll(chatHistory.sublist(historyStart));

      // 3. Current User Message (WITH RAG CONTEXT)
      final userMessage = _buildUserMessage(query, contextText, mode, language);
      messages.add(Message(role: MessageRole.user, content: userMessage));

      // Save raw query to history (not the huge context prompt)
      onHistoryUpdate?.call(Message(role: MessageRole.user, content: query));

      // Debug: Log prompt structure
      debugPrint('ğŸ“¨ === Prompt to LLM ===');
      debugPrint('ğŸ“¨ System: ${messages[0].content}');
      debugPrint('ğŸ“¨ History: ${chatHistory.length} messages');
      debugPrint('ğŸ“¨ User Query: $query');
      debugPrint('ğŸ“¨ Context Length: ${contextText.length} chars');
      debugPrint('ğŸ“¨ Mode: ${mode.name}');
      debugPrint('ğŸ“¨ Language: ${language.name}');

      // Stream response from Ollama
      final responseBuffer = StringBuffer();
      final thinkingBuffer = StringBuffer();
      bool isInThinking = false;
      int chunkCount = 0;

      debugPrint('ğŸ“ === LLM Streaming Start ===');

      final stream = ollamaClient.generateChatCompletionStream(
        request: GenerateChatCompletionRequest(
          model: modelName,
          messages: messages,
        ),
      );

      await for (final chunk in stream) {
        final content = chunk.message.content;
        responseBuffer.write(content);
        chunkCount++;

        // Detect thinking/reasoning sections (some models use <think> tags)
        if (content.contains('<think>')) {
          isInThinking = true;
          debugPrint('ğŸ§  [THINKING START]');
        }
        if (content.contains('</think>')) {
          isInThinking = false;
          debugPrint('ğŸ§  [THINKING END]');
        }

        // Log chunk content
        if (isInThinking) {
          thinkingBuffer.write(content);
          // Print thinking chunks with special prefix
          final cleanContent = content.replaceAll('\n', 'â†•');
          debugPrint('ğŸ§  $cleanContent');
        } else {
          // Stream token to UI callback (real-time display)
          if (onToken != null && content.isNotEmpty) {
            onToken(content);
          }
          // Print response chunks
          final cleanContent = content.replaceAll('\n', 'â†•');
          if (cleanContent.isNotEmpty) {
            // debugPrint('ğŸ’¬ $cleanContent');
          }
        }
      }

      debugPrint('ğŸ“ === LLM Streaming End ($chunkCount chunks) ===');

      // Log thinking summary if any
      if (thinkingBuffer.isNotEmpty) {
        debugPrint('ğŸ§  === Thinking Summary ===');
        debugPrint(thinkingBuffer.toString());
        debugPrint('ğŸ§  === End Thinking ===');
      }

      final response = responseBuffer.toString().trim();

      // Save assistant response to history
      onHistoryUpdate?.call(
        Message(role: MessageRole.assistant, content: response),
      );

      if (response.isEmpty) {
        return OllamaResponseResult(
          response:
              'âš ï¸ The model returned an empty response. Please try again.',
          mode: mode,
          bestSimilarity: bestSimilarity,
        );
      }

      return OllamaResponseResult(
        response: response,
        mode: mode,
        bestSimilarity: bestSimilarity,
      );
    } catch (e, stackTrace) {
      debugPrint('ğŸ”´ Ollama Error: $e');
      debugPrint('ğŸ”´ Stack Trace: $stackTrace');

      return OllamaResponseResult(
        response:
            'âš ï¸ Ollama Error: $e\n\n'
            'Make sure Ollama is running (ollama serve) and the model is installed.',
        mode: mode,
        bestSimilarity: bestSimilarity,
      );
    }
  }

  /// Calculate best similarity score from RAG results
  double _calculateBestSimilarity(RagSearchResult ragResult) {
    if (ragResult.chunks.isEmpty) return 0.0;

    return ragResult.chunks
        .map((c) => c.similarity)
        .where((s) => s > 0) // Exclude adjacent chunks with 0.0
        .fold(0.0, (a, b) => a > b ? a : b);
  }

  /// Determine response mode based on context and similarity
  ResponseMode _determineResponseMode(
    bool hasRelevantContext,
    double bestSimilarity,
  ) {
    if (!hasRelevantContext) return ResponseMode.fallback;
    if (bestSimilarity >= strictThreshold) return ResponseMode.strict;
    if (bestSimilarity >= hybridThreshold) return ResponseMode.hybrid;
    return ResponseMode.fallback;
  }

  /// Build system prompt based on response mode and language
  Message _buildSystemPrompt(ResponseMode mode, ResponseLanguage language) {
    if (language == ResponseLanguage.korean) {
      return _buildKoreanSystemPrompt(mode);
    }
    return _buildEnglishSystemPrompt(mode);
  }

  Message _buildEnglishSystemPrompt(ResponseMode mode) {
    switch (mode) {
      case ResponseMode.strict:
        return const Message(
          role: MessageRole.system,
          content:
              'You are an AI assistant that answers accurately based on the provided context. '
              'Prioritize information from the context in your answers.',
        );
      case ResponseMode.hybrid:
        return const Message(
          role: MessageRole.system,
          content:
              'You are an AI assistant that combines the provided context with general knowledge. '
              'Prioritize context information, but supplement with general knowledge when needed. '
              'Clearly distinguish between information from the context and general knowledge.',
        );
      case ResponseMode.fallback:
        return const Message(
          role: MessageRole.system,
          content: 'You are a helpful AI assistant.',
        );
    }
  }

  Message _buildKoreanSystemPrompt(ResponseMode mode) {
    switch (mode) {
      case ResponseMode.strict:
        return const Message(
          role: MessageRole.system,
          content:
              'ë‹¹ì‹ ì€ ì œê³µëœ ë¬¸ë§¥ì„ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê²Œ ë‹µë³€í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. '
              'ë‹µë³€ ì‹œ ì œê³µëœ ë¬¸ì„œì˜ ë‚´ìš©ì„ ìµœìš°ì„ ìœ¼ë¡œ ì°¸ê³ í•˜ì„¸ìš”. í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.',
        );
      case ResponseMode.hybrid:
        return const Message(
          role: MessageRole.system,
          content:
              'ë‹¹ì‹ ì€ ì œê³µëœ ë¬¸ë§¥ê³¼ ì¼ë°˜ ì§€ì‹ì„ ì¡°í•©í•˜ì—¬ ë‹µë³€í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. '
              'ë¬¸ì„œì˜ ì •ë³´ë¥¼ ìš°ì„ ì‹œí•˜ë˜, í•„ìš”í•œ ê²½ìš° ì¼ë°˜ ì§€ì‹ì„ ë³´ì¶©í•˜ì—¬ ì„¤ëª…í•˜ì„¸ìš”. '
              'ë¬¸ì„œì—ì„œ ì°¾ì€ ë‚´ìš©ê³¼ ì¼ë°˜ ì§€ì‹ì„ êµ¬ë¶„í•˜ì—¬ ë‹µë³€í•´ì£¼ì„¸ìš”. í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.',
        );
      case ResponseMode.fallback:
        return const Message(
          role: MessageRole.system,
          content: 'ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.',
        );
    }
  }

  /// Build user message with context based on response mode and language
  String _buildUserMessage(
    String query,
    String contextText,
    ResponseMode mode,
    ResponseLanguage language,
  ) {
    if (language == ResponseLanguage.korean) {
      return _buildKoreanUserMessage(query, contextText, mode);
    }
    return _buildEnglishUserMessage(query, contextText, mode);
  }

  String _buildEnglishUserMessage(
    String query,
    String contextText,
    ResponseMode mode,
  ) {
    switch (mode) {
      case ResponseMode.strict:
        return '''
[Reference Documents]
$contextText
[End of Reference Documents]

Based on the content above, please answer the following question.

Question: $query''';

      case ResponseMode.hybrid:
        return '''
[Related Documents]
$contextText
[End of Related Documents]

The documents above contain related information. Please answer based on the document content, 
but you may supplement with general knowledge if needed.
Please distinguish between information from the documents and general knowledge.

Question: $query''';

      case ResponseMode.fallback:
        return '''
Question: $query

Note: No directly relevant information was found in the uploaded documents.
Please answer with general knowledge, and suggest adding relevant documents if more accurate information is needed.''';
    }
  }

  String _buildKoreanUserMessage(
    String query,
    String contextText,
    ResponseMode mode,
  ) {
    switch (mode) {
      case ResponseMode.strict:
        return '''
[ì°¸ê³  ë¬¸ì„œ]
$contextText
[ì°¸ê³  ë¬¸ì„œ ë]

ìœ„ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ ë‹µë³€í•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: $query''';

      case ResponseMode.hybrid:
        return '''
[ê´€ë ¨ ë¬¸ì„œ]
$contextText
[ê´€ë ¨ ë¬¸ì„œ ë]

ìœ„ ë¬¸ì„œëŠ” ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë‚´ìš©ì„ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤. ë¬¸ì„œì˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ë˜, 
í•„ìš”í•˜ë‹¤ë©´ ì¼ë°˜ ì§€ì‹ì„ ì¶”ê°€í•˜ì—¬ ì„¤ëª…í•´ì£¼ì„¸ìš”.
ë¬¸ì„œì˜ ë‚´ìš©ê³¼ ì¼ë°˜ ì§€ì‹ì„ êµ¬ë¶„í•˜ì—¬ ë‹µë³€í•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: $query''';

      case ResponseMode.fallback:
        return '''
ì§ˆë¬¸: $query

ì°¸ê³ : ì—…ë¡œë“œëœ ë¬¸ì„œì—ì„œ ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.
ì¼ë°˜ ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì‹œê³ , ë” ì •í™•í•œ ì •ë³´ê°€ í•„ìš”í•˜ë‹¤ë©´ ê´€ë ¨ ë¬¸ì„œë¥¼ ì¶”ê°€í•˜ë„ë¡ ì•ˆë‚´í•´ì£¼ì„¸ìš”.''';
    }
  }
}
